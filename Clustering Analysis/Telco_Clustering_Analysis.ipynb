{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTldK9W0UEa8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'Raw DataSet.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = data.isnull().sum()\n",
        "\n",
        "# Handle missing values (if any)\n",
        "data = data.dropna()  # For simplicity, dropping rows with missing values\n",
        "\n",
        "# Encoding categorical variables\n",
        "categorical_columns = ['gender', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'Contract', 'Churn']\n",
        "data_encoded = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X = data_encoded.drop('MonthlyCharges', axis=1)\n",
        "y = data_encoded['MonthlyCharges']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scaling the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "#import ace_tools as tools; tools.display_dataframe_to_user(name=\"Pre-processed Dataset\", dataframe=pd.DataFrame(X_train_scaled))\n",
        "\n",
        "# Display the first few rows of the scaled training data\n",
        "pd.DataFrame(X_train_scaled).head()\n"
      ],
      "metadata": {
        "id": "h4GJLt4WUuuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the pre-processed, training, and testing datasets for download\n",
        "pre_processed_path = 'pre_processed_dataset.csv'\n",
        "training_set_path = 'training_set.csv'\n",
        "testing_set_path = 'testing_set.csv'\n",
        "\n",
        "# Saving the pre-processed dataset\n",
        "data_encoded.to_csv(pre_processed_path, index=False)\n",
        "\n",
        "# Saving the training set\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
        "training_set = pd.concat([X_train_scaled_df, y_train.reset_index(drop=True)], axis=1)\n",
        "training_set.to_csv(training_set_path, index=False)\n",
        "\n",
        "# Saving the testing set\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
        "testing_set = pd.concat([X_test_scaled_df, y_test.reset_index(drop=True)], axis=1)\n",
        "testing_set.to_csv(testing_set_path, index=False)\n",
        "\n",
        "pre_processed_path, training_set_path, testing_set_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24WObxjqbVMW",
        "outputId": "71a21589-c322-4238-9468-09537cb0ec85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('pre_processed_dataset.csv', 'training_set.csv', 'testing_set.csv')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of samples in the original dataset\n",
        "total_samples = data_encoded.shape[0]\n",
        "\n",
        "# Get the number of samples in the training and testing sets\n",
        "training_samples = X_train.shape[0]\n",
        "testing_samples = X_test.shape[0]\n",
        "\n",
        "total_samples, training_samples, testing_samples\n"
      ],
      "metadata": {
        "id": "mbR4Uv-Ig6Hp",
        "outputId": "8feffa09-08cb-4c0a-addc-8ad293a005f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7043, 5634, 1409)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Compute the correlation matrix\n",
        "correlation_matrix = data_encoded.corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=plt.cm.Reds)\n",
        "plt.show()\n",
        "\n",
        "# Identify highly correlated features\n",
        "threshold = 0.75\n",
        "high_corr_pairs = [(i, j) for i in correlation_matrix.columns for j in correlation_matrix.columns\n",
        "                   if i != j and correlation_matrix.loc[i, j] > threshold]\n",
        "\n",
        "# Train a RandomForest model to get feature importances\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train_scaled, y_train)\n",
        "feature_importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(12, 8))\n",
        "feature_importances.sort_values().plot(kind='barh')\n",
        "plt.show()\n",
        "\n",
        "# Selecting important features\n",
        "selected_features = feature_importances[feature_importances > 0.01].index\n",
        "X_train_selected = X_train_scaled[:, feature_importances > 0.01]\n",
        "X_test_selected = X_test_scaled[:, feature_importances > 0.01]\n",
        "\n",
        "#import ace_tools as tools; tools.display_dataframe_to_user(name=\"Selected Features\", dataframe=pd.DataFrame(X_train_selected))\n",
        "\n",
        "# Generate polynomial features\n",
        "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
        "X_train_poly = poly.fit_transform(X_train_selected)\n",
        "X_test_poly = poly.transform(X_test_selected)\n",
        "\n",
        "#tools.display_dataframe_to_user(name=\"Polynomial Features\", dataframe=pd.DataFrame(X_train_poly))\n",
        "\n",
        "# Display the first few rows of the generated polynomial features\n",
        "pd.DataFrame(X_train_poly).head()\n"
      ],
      "metadata": {
        "id": "DBHUz99fbcTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Determine the optimal number of clusters using the elbow method\n",
        "wcss = []\n",
        "max_clusters = 10\n",
        "\n",
        "for i in range(1, max_clusters + 1):\n",
        "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=42)\n",
        "    kmeans.fit(X_train_poly)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the elbow method graph\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, max_clusters + 1), wcss, marker='o', linestyle='--')\n",
        "plt.title('Elbow Method for Optimal Number of Clusters')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('WCSS')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CfdJ7SO_IBqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Train the K-Means clustering model with 3 clusters\n",
        "kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300, n_init=10, random_state=42)\n",
        "clusters = kmeans.fit_predict(X_train_poly)\n",
        "\n",
        "# Add the cluster labels to the dataset\n",
        "X_train_clustered = pd.DataFrame(X_train_poly)\n",
        "X_train_clustered['Cluster'] = clusters\n",
        "\n",
        "# Visualize the clusters\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=X_train_clustered, x=0, y=1, hue='Cluster', palette='viridis')\n",
        "plt.title('Clusters Visualization')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()\n",
        "\n",
        "#import ace_tools as tools; tools.display_dataframe_to_user(name=\"Clustered Dataset\", dataframe=X_train_clustered)\n"
      ],
      "metadata": {
        "id": "jY7BHG--IlHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Apply PCA to reduce to 2 dimensions for visualization\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_poly)\n",
        "\n",
        "# Add the cluster labels to the PCA-transformed data\n",
        "X_train_pca_df = pd.DataFrame(X_train_pca, columns=['PCA1', 'PCA2'])\n",
        "X_train_pca_df['Cluster'] = clusters\n",
        "\n",
        "# Visualize the clusters in the PCA-transformed space\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=X_train_pca_df, x='PCA1', y='PCA2', hue='Cluster', palette='viridis')\n",
        "plt.title('Clusters Visualization with PCA')\n",
        "plt.xlabel('PCA1')\n",
        "plt.ylabel('PCA2')\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "U9m086r8I3g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Retrieve the centroids of the clusters\n",
        "centroids = kmeans.cluster_centers_\n",
        "\n",
        "# No need to apply PCA inverse transform as centroids are already in the original feature space\n",
        "centroids_df = pd.DataFrame(centroids, columns=[f'Feature{i+1}' for i in range(centroids.shape[1])])\n",
        "\n",
        "# Add cluster labels\n",
        "centroids_df['Cluster'] = [0, 1, 2]\n",
        "\n",
        "centroids_df"
      ],
      "metadata": {
        "id": "NWAZw9lHJVQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a dataframe with the original features and cluster labels\n",
        "X_train_original_df = pd.DataFrame(X_train, columns=X.columns)\n",
        "X_train_original_df['Cluster'] = clusters\n",
        "\n",
        "# Visualize the distribution of key features within each cluster\n",
        "for feature in X_train_original_df.columns[:-1]:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(x='Cluster', y=feature, data=X_train_original_df, palette='viridis')\n",
        "    plt.title(f'Distribution of {feature} by Cluster')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "kDVKI63IJdmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a pair plot of the key features colored by cluster\n",
        "sns.pairplot(X_train_original_df, hue='Cluster', palette='viridis', diag_kind='kde')\n",
        "plt.suptitle('Pair Plot of Key Features by Cluster', y=1.02)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GJ-vGpzCKaYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of key features to visualize\n",
        "key_features = ['tenure', 'MonthlyCharges']  # Verify if 'MonthlyCharges' is the correct column name\n",
        "\n",
        "for feature in key_features:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    # Check if the feature exists in the DataFrame before plotting\n",
        "    if feature in X_train_original_df.columns:\n",
        "        sns.histplot(data=X_train_original_df, x=feature, hue='Cluster', multiple='stack', palette='viridis', kde=True)\n",
        "        plt.title(f'Distribution of {feature} by Cluster')\n",
        "        plt.xlabel(feature)\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"Warning: Feature '{feature}' not found in the DataFrame.\")"
      ],
      "metadata": {
        "id": "7_L9QHQWKheA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a heatmap for the entire dataset to see the correlation between features\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(X_train_original_df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Heatmap of Feature Correlations')\n",
        "plt.show()\n",
        "\n",
        "# Create heatmaps for each cluster to see the correlation within clusters\n",
        "for cluster in X_train_original_df['Cluster'].unique():\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(X_train_original_df[X_train_original_df['Cluster'] == cluster].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "    plt.title(f'Heatmap of Feature Correlations for Cluster {cluster}')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "8wWiSiJ2Kkxe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}