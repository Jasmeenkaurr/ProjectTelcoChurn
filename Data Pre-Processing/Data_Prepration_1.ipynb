{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTldK9W0UEa8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'Raw DataSet.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "QtGnUyDQLibZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = data.isnull().sum()\n",
        "\n",
        "# Handle missing values (if any)\n",
        "data = data.dropna()  # For simplicity, dropping rows with missing values\n",
        "\n",
        "# Encoding categorical variables\n",
        "categorical_columns = ['gender', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'Contract', 'Churn']\n",
        "data_encoded = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X = data_encoded.drop('MonthlyCharges', axis=1)\n",
        "y = data_encoded['MonthlyCharges']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scaling the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "#import ace_tools as tools; tools.display_dataframe_to_user(name=\"Pre-processed Dataset\", dataframe=pd.DataFrame(X_train_scaled))\n",
        "\n",
        "# Display the first few rows of the scaled training data\n",
        "pd.DataFrame(X_train_scaled).head()\n"
      ],
      "metadata": {
        "id": "h4GJLt4WUuuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the pre-processed, training, and testing datasets for download\n",
        "pre_processed_path = 'pre_processed_dataset.csv'\n",
        "training_set_path = 'training_set.csv'\n",
        "testing_set_path = 'testing_set.csv'\n",
        "\n",
        "# Saving the pre-processed dataset\n",
        "data_encoded.to_csv(pre_processed_path, index=False)\n",
        "\n",
        "# Saving the training set\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
        "training_set = pd.concat([X_train_scaled_df, y_train.reset_index(drop=True)], axis=1)\n",
        "training_set.to_csv(training_set_path, index=False)\n",
        "\n",
        "# Saving the testing set\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
        "testing_set = pd.concat([X_test_scaled_df, y_test.reset_index(drop=True)], axis=1)\n",
        "testing_set.to_csv(testing_set_path, index=False)\n",
        "\n",
        "pre_processed_path, training_set_path, testing_set_path\n"
      ],
      "metadata": {
        "id": "24WObxjqbVMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of samples in the original dataset\n",
        "total_samples = data_encoded.shape[0]\n",
        "\n",
        "# Get the number of samples in the training and testing sets\n",
        "training_samples = X_train.shape[0]\n",
        "testing_samples = X_test.shape[0]\n",
        "\n",
        "total_samples, training_samples, testing_samples\n"
      ],
      "metadata": {
        "id": "mbR4Uv-Ig6Hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Compute the correlation matrix\n",
        "correlation_matrix = data_encoded.corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=plt.cm.Reds)\n",
        "plt.show()\n",
        "\n",
        "# Identify highly correlated features\n",
        "threshold = 0.75\n",
        "high_corr_pairs = [(i, j) for i in correlation_matrix.columns for j in correlation_matrix.columns\n",
        "                   if i != j and correlation_matrix.loc[i, j] > threshold]\n",
        "\n",
        "# Train a RandomForest model to get feature importances\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train_scaled, y_train)\n",
        "feature_importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(12, 8))\n",
        "feature_importances.sort_values().plot(kind='barh')\n",
        "plt.show()\n",
        "\n",
        "# Selecting important features\n",
        "selected_features = feature_importances[feature_importances > 0.01].index\n",
        "X_train_selected = X_train_scaled[:, feature_importances > 0.01]\n",
        "X_test_selected = X_test_scaled[:, feature_importances > 0.01]\n",
        "\n",
        "#import ace_tools as tools; tools.display_dataframe_to_user(name=\"Selected Features\", dataframe=pd.DataFrame(X_train_selected))\n",
        "\n",
        "# Generate polynomial features\n",
        "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
        "X_train_poly = poly.fit_transform(X_train_selected)\n",
        "X_test_poly = poly.transform(X_test_selected)\n",
        "\n",
        "#tools.display_dataframe_to_user(name=\"Polynomial Features\", dataframe=pd.DataFrame(X_train_poly))\n",
        "\n",
        "# Display the first few rows of the generated polynomial features\n",
        "pd.DataFrame(X_train_poly).head()\n"
      ],
      "metadata": {
        "id": "DBHUz99fbcTc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}